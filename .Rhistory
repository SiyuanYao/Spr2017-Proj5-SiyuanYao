##########Data Exploration
library(maps)
map<- read_csv("~/map ada.csv")
map$colorBuckets <- as.numeric(cut(map$crime, c(30, 50, 70, 90, 110, 100)))
colorsmatched <- map$colorBuckets[match(county.fips$fips, map$fips)]
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0,
lty = 0, projection = "polyconic")
# Plot Crime Ratio by country
colors = c("#F1EEF6", "#D4B9DA", "#C994 C7", "#DF65B0", "#DD1C77",
"#980043")
map("state", col = "white", fill = FALSE, add = TRUE, lty = 1, lwd = 0.2,
projection = "polyconic")
title("Crime Ratio by county, 1990")
leg.txt <- c("<30%", "30-50%", "50-70%", "70-90%", "90-110%", ">110%")
legend("topright", leg.txt, horiz = TRUE, fill = colors)
#Heat map correlation
library(reshape2)
library(Hmisc)
library(stats)
abbreviateSTR <- function(value, prefix){  # format string more concisely
lst = c()
for (item in value) {
if (is.nan(item) || is.na(item)) { # if item is NaN return empty string
lst <- c(lst, '')
next
}
item <- round(item, 2) # round to two digits
if (item == 0) { # if rounding results in 0 clarify
item = '<.01'
}
item <- as.character(item)
item <- sub("(^[0])+", "", item)    # remove leading 0: 0.05 -> .05
item <- sub("(^-[0])+", "-", item)  # remove leading -0: -0.05 -> -.05
lst <- c(lst, paste(prefix, item, sep = ""))
}
return(lst)
}
cordata=data[,4:17]
d <- cordata
cormatrix = rcorr(as.matrix(d), type='spearman')
cordata = melt(cormatrix$r)
cordata$labelr = abbreviateSTR(melt(cormatrix$r)$value, 'r')
cordata$labelP = abbreviateSTR(melt(cormatrix$P)$value, 'P')
cordata$label = paste(cordata$labelr, "\n",
cordata$labelP, sep = "")
cordata$strike = ""
cordata$strike[cormatrix$P > 0.05] = "X"
txtsize <- par('din')[2] / 2
ggplot(cordata, aes(x=Var1, y=Var2, fill=value)) + geom_tile() +
theme(axis.text.x = element_text(angle=90, hjust=TRUE)) +
xlab("") + ylab("") +
geom_text(label=cordata$label, size=txtsize) +
geom_text(label=cordata$strike, size=txtsize * 4, color="red", alpha=0.4)
ggplot(cordata, aes(x=Var1, y=Var2, fill=value)) +
geom_tile() + xlab("") + ylab("")
##########Raw Model building
attach(data)
head(data)
rawmod=lm(CrimeRatio~PopuDensity+log(Population)	+Population1834+	Population66+Physician+HospitalBed+HighSchool+	Bachelor+Poverty+Unemployment+log(PerCapitaIncome)+log(PersonIncome)+	factor(Region)
,data)
summary(rawmod)
par(mfrow=c(2,2))
install.packages('ggfortify')
library(ggplot2)
library(ggfortify)
autoplot(rawmod)
par(mfrow = c(1, 2))
autoplot(rawmod, which = 1:6, ncol = 3, label.size = 3)
############Remove Outliers
data=data[-125,]
mod=lm(CrimeRatio~PopuDensity+log(Population)	+Population1834+	Population66+Physician+HospitalBed+HighSchool+	Bachelor+Poverty+Unemployment+log(PerCapitaIncome)+log(PersonIncome)+	factor(Region)
,data)
summary(mod)
par(mfrow=c(1,1))
hist(mod$resi)
par(mfrow=c(2,2))
autoplot(mod, which = 1:6, ncol = 3, label.size = 3)
mod=lm(CrimeRatio~PopuDensity+log(Population)	+Population1834+	Population66+Physician+HospitalBed+HighSchool+	Bachelor+Poverty+Unemployment+log(PerCapitaIncome)+log(PersonIncome)+	factor(Region)
,data)
autoplot(mod, which = 1:6, ncol = 3, label.size = 3)
data=data[-190,]
mod=lm(CrimeRatio~PopuDensity+log(Population)	+Population1834+	Population66+Physician+HospitalBed+HighSchool+	Bachelor+Poverty+Unemployment+log(PerCapitaIncome)+log(PersonIncome)+	factor(Region)
,data)
autoplot(mod, which = 1:6, ncol = 3, label.size = 3)
############Model Selection
library(MASS)
step <- stepAIC(mod, direction="both")
mod=lm(CrimeRatio ~ log(Population) + Population1834 + HospitalBed +
Bachelor + Poverty + log(PersonIncome) + factor(Region),data)
summary(mod)
vif(mod)
predict(mod, data, interval="confidence")
############Test Data
testdata=testdata[-91,]
mod=lm(CrimeRatio ~ log(Population) + Population1834 + HospitalBed +
Bachelor + Poverty + log(PersonIncome) + factor(Region),testdata)
summary(mod)
autoplot(mod, which = 1:6, ncol = 3, label.size = 3)
#############CV
library(DAAG)
cv.lm(data =data, mod, m=3) # 3 fold cross-validation
library(bootstrap)
# define functions
theta.fit <- function(x,y){lsfit(x,y)}
theta.predict <- function(mod,x){cbind(1,x)%*%mod$coef}
# matrix of predictors
X <- as.matrix(data[c("Population", "Poverty","PersonIncome","Population1834","PersonIncome")])
# vector of predicted values
y <- as.matrix(data[c("CrimeRatio")])
results <- crossval(X,y,theta.fit,theta.predict,ngroup=10)
cor(y,results$cv.fit)**2 # cross-validated R2
confint(mod, data, interval="confidence")
confint(mod, data)
confint(mod)
mod=lm(CrimeRatio ~ + Population1834 + HospitalBed +
Bachelor + Poverty + factor(Region),data)
summary(mod)
vif(mod)
confint(mod)
theta.fit <- function(x,y){lsfit(x,y)}
theta.predict <- function(mod,x){cbind(1,x)%*%mod$coef}
# matrix of predictors
X <- as.matrix(data[c("Population", "Poverty","PersonIncome","Population1834","PersonIncome")])
# vector of predicted values
y <- as.matrix(data[c("CrimeRatio")])
results <- crossval(X,y,theta.fit,theta.predict,ngroup=10)
cor(y,results$cv.fit)**2 # cross-validated R2
library(DAAG)
cv.lm(data =data, mod, m=3) # 3 fold cross-validation
library(bootstrap)
mse(mod)
sm<-summary(mod)
mse <- function(sm)
mean(sm$residuals^2)
mse
mean((testdata - predict(mod))^2)
shiny::runApp('D:/Spr2017-proj5-grp7/app')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
source('D:/h1b_visa_shiny/helpers.R', echo=TRUE)
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
source('D:/h1b_visa_shiny/helpers.R', echo=TRUE)
g <- ggplot(USA, aes(x=long, y=lat)) +
geom_polygon() + xlab("Longitude (deg)") + ylab("Latitude(deg)") +
geom_point(data=map_df, aes_string(x="lon", y="lat", label = "WORKSITE", alpha = metric, size = metric), color="red") +
geom_label_repel(data=map_df %>% filter(WORKSITE %in% top_locations),aes_string(x="lon", y="lat",label = "WORKSITE"),
fontface = 'bold', color = 'black',
box.padding = unit(0.0, "lines"),
point.padding = unit(1.0, "lines"),
segment.color = 'grey50',
force = 3) +
# Zoom into the specific location input
#coord_map(ylim = c(max(geo_coord$lat_min - 5,23), min(geo_coord$lat_max - 5,50)),xlim=c(max(geo_coord$long_min - 5,-130),min(geo_coord$long_max + 5,-65))) +
# Using the whole USA map
coord_map(ylim = c(23,50),xlim=c(-130,-65)) +
get_theme()
USA
g <- ggplot(USA, aes(x=long, y=lat)) +
geom_polygon() + xlab("Longitude (deg)") + ylab("Latitude(deg)") +
geom_point(data=map_df, aes_string(x="lon", y="lat", label = "WORKSITE", alpha = metric, size = metric), color="yellow") +
geom_label_repel(data=map_df %>% filter(WORKSITE %in% top_locations),aes_string(x="lon", y="lat",label = "WORKSITE"),
fontface = 'bold', color = 'black',
box.padding = unit(0.0, "lines"),
point.padding = unit(1.0, "lines"),
segment.color = 'grey50',
force = 3) +
# Zoom into the specific location input
#coord_map(ylim = c(max(geo_coord$lat_min - 5,23), min(geo_coord$lat_max - 5,50)),xlim=c(max(geo_coord$long_min - 5,-130),min(geo_coord$long_max + 5,-65))) +
# Using the whole USA map
coord_map(ylim = c(23,50),xlim=c(-130,-65)) +
get_theme()
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
convert("h1b_transformed.rds", "h1b_transformed.rdata")
library("rio")
install.packages("rio")
convert("h1b_transformed.rds", "h1b_transformed.rdata")
library("rio")
runApp('D:/h1b_visa_shiny')
source("https://bioconductor.org/biocLite.R")
biocLite("convert")
convert("h1b_transformed.rds","h1b_transformed.Rdata")
a]
runApp('D:/Spr2017-proj5-grp7/app')
library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
setwd("D:/Spr2017-proj5-grp7")
load("D:/Spr2017-proj5-grp7/output/cleaned_data.Rdata")
colnames(h1b)
print("Column matching ..")
{
h1b= h1b %>%
mutate(CASE_NUMBER = X,
#CASE_STATUS = CASE_STATUS,
#EMPLOYER_NAME = EMPLOYER_NAME,
#SOC_NAME = SOC_NAME,
#JOB_TITLE = JOB_TITLE,
#FULL_TIME_POSITION = FULL_TIME_POSITION,
#PREVAILING_WAGE = PREVAILING_WAGE,
WORKSITE_CITY = CITY,
WORKSITE_STATE = STATE
)
}
# Selecting only the relevant columns
h1b = h1b %>%
select(CASE_NUMBER,
CASE_STATUS,
EMPLOYER_NAME,
SOC_NAME,
JOB_TITLE,
FULL_TIME_POSITION,
PREVAILING_WAGE,
WORKSITE_CITY,
WORKSITE_STATE,
YEAR,
lon,
lat)
#Full time position distribution
#For filling the missing values,  then analyze the relationship of the Prevailing Wage with Full Time Position across the years.
# Generic ggplot graphics configuration I will be using for all my plots
state_abbs = c("AK", "AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA",
"HI", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME",
"MI", "MN", "MO", "MS",  "MT", "NC", "ND", "NE", "NH", "NJ", "NM",
"NV", "NY", "OH", "OK", "OR", "PA", "PR", "RI", "SC", "SD", "TN",
"TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")
state_full = c("alaska","alabama","arkansas","arizona","california","colorado",
"connecticut","district of columbia","delaware","florida","georgia",
"hawaii","iowa","idaho","illinois","indiana","kansas","kentucky",
"louisiana","massachusetts","maryland","maine","michigan","minnesota",
"missouri","mississippi","montana","north carolina","north dakota",
"nebraska","new hampshire","new jersey","new mexico","nevada",
"new york","ohio","oklahoma","oregon","pennsylvania","puerto rico",
"rhode island","south carolina","south dakota","tennessee","texas",
"utah","virginia","vermont","washington","wisconsin",
"west virginia","wyoming")
state_hash = hashmap(state_abbs,state_full)
h1b$WORKSITE_STATE_FULL = sapply(h1b$WORKSITE_STATE, function(x,y) {return(toupper(y[[x]]))}, y = state_hash)
library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
setwd("D:/Spr2017-proj5-grp7")
load("D:/Spr2017-proj5-grp7/output/cleaned_data.Rdata")
colnames(h1b)
print("Column matching ..")
{
h1b= h1b %>%
mutate(CASE_NUMBER = X,
#CASE_STATUS = CASE_STATUS,
#EMPLOYER_NAME = EMPLOYER_NAME,
#SOC_NAME = SOC_NAME,
#JOB_TITLE = JOB_TITLE,
#FULL_TIME_POSITION = FULL_TIME_POSITION,
#PREVAILING_WAGE = PREVAILING_WAGE,
WORKSITE_CITY = CITY,
WORKSITE_STATE = STATE
)
}
# Selecting only the relevant columns
h1b = h1b %>%
select(CASE_NUMBER,
CASE_STATUS,
EMPLOYER_NAME,
SOC_NAME,
JOB_TITLE,
FULL_TIME_POSITION,
PREVAILING_WAGE,
WORKSITE_CITY,
WORKSITE_STATE,
YEAR,
lon,
lat)
#Full time position distribution
#For filling the missing values,  then analyze the relationship of the Prevailing Wage with Full Time Position across the years.
# Generic ggplot graphics configuration I will be using for all my plots
state_abbs = c("AK", "AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA",
"HI", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME",
"MI", "MN", "MO", "MS",  "MT", "NC", "ND", "NE", "NH", "NJ", "NM",
"NV", "NY", "OH", "OK", "OR", "PA", "PR", "RI", "SC", "SD", "TN",
"TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")
state_full = c("alaska","alabama","arkansas","arizona","california","colorado",
"connecticut","district of columbia","delaware","florida","georgia",
"hawaii","iowa","idaho","illinois","indiana","kansas","kentucky",
"louisiana","massachusetts","maryland","maine","michigan","minnesota",
"missouri","mississippi","montana","north carolina","north dakota",
"nebraska","new hampshire","new jersey","new mexico","nevada",
"new york","ohio","oklahoma","oregon","pennsylvania","puerto rico",
"rhode island","south carolina","south dakota","tennessee","texas",
"utah","virginia","vermont","washington","wisconsin",
"west virginia","wyoming")
state_hash = hashmap(state_abbs,state_full)
h1b$WORKSITE_STATE_FULL = sapply(h1b$WORKSITE_STATE, function(x,y) {return(toupper(y[[x]]))}, y = state_hash)
site_merge <- function(x,y) {
return(paste0(x,", ",y))
}
h1b %>%
rename(WORKSITE_STATE_ABB = WORKSITE_STATE) -> h1b
h1b$WORKSITE = mapply(site_merge,h1b$WORKSITE_CITY,h1b$WORKSITE_STATE_FULL)
wrong_names = c("NEW YROK, NEW YORK", "SUUNYVALE, CALIFORNIA", "SAN FRANSISCO, CALIFORNIA")
h1b %>%
filter(WORKSITE %in% wrong_names) %>%
group_by(WORKSITE) %>%
summarise(count = n())
h1b %>%
group_by(WORKSITE) %>%
summarise(count = n()) %>%
arrange(desc(count)) -> sites_count
site_hash = hashmap(sites_count$WORKSITE, sites_count$count)
get_inserts <- function(split_left,split_right, i, letters) {
# Generate insertions of a single letter
return(unlist(sapply(letters, function(left,right,c) {return(paste0(left, c, right))}, left = split_left[i], right = split_right[i])))
}
get_deletes <- function(split_left,split_right, i) {
# Generate deletion of one letter from word
return(paste0(split_left[i], substr(split_right[i],2,nchar(split_right[i]))))
}
get_replaces <- function(split_left,split_right, i,letters) {
# Generate replacement of a letter by a-z or space
if(!is.null(split_right[i]) &  nchar(split_right[i]) > 0) {
return(unlist(sapply(letters, function(left,right,c) {return(paste0(left, c, right))}, left = split_left[i], right = substr(split_right[i],2,nchar(split_right[i])))))
}
return(NULL)
}
get_transposes <- function(split_left, split_right,i) {
# Generate interchanging of the positions of adjacent letters
if(!is.null(split_right[i]) & nchar(split_right[i]) > 1) {
return(paste0(split_left[i],substr(split_right[i],2,2),substr(split_right[i],1,1),substr(split_right[i],3,nchar(split_right[i]))))
}
return(NULL)
}
edits1site <- function(site) {
# All edits that are one edit away from site
letters = toupper(strsplit("abcdefghijklmnopqrstuvwxyz ",split='')[[1]])
site_len <- nchar(site)
#print(site_len)
if(site_len < 4) {
return(site)
}
split_left <- sapply(seq(0,site_len), substr,x = site,start = 1)
split_right <- sapply(seq(1,site_len+1), substr,x = site,stop = site_len)
deletes <- sapply(seq(1,site_len+1),get_deletes, split_left = split_left, split_right = split_right)
transposes <- unlist(sapply(seq(1,site_len+1),get_transposes, split_left = split_left, split_right = split_right))
replaces <- unlist(sapply(seq(1,site_len+1),get_replaces, split_left = split_left, split_right = split_right, letters=letters))
inserts <- unlist(sapply(seq(1,site_len+1),get_inserts, split_left = split_left, split_right = split_right,letters = letters))
return(unique(c(deletes,transposes,replaces,inserts)))
}
edits2site <- function(site) {
# All edits that are two edits away from `word`
edits1_sites = edits1site(site)
return (unlist(sapply(edits1_sites, edits1site)))
}
get_prob <- function(site, site_hash) {
# probability of site in our dataset
return(site_hash[[site]])
}
known <- function(sites,site_hash = site_hash) {
# The subset of candidate sites that appear in the dictionary of sites
return(sites[site_hash$has_keys(sites)])
}
find_candidates <- function(site,...) {
# Generate possible spelling corrections for word
return(c(known(site,...), known(edits1site(site),...), c(site)))
}
site_spell_correcter <- function(site,...) {
# best possible correction to the site
candidates = find_candidates(site,...)
best_candi = candidates[which.max(sapply(candidates,get_prob, ...))]
#if(get_prob(best_candi,...) > get_prob(site,...) ) {
#  return(best_candi)
#}
return(best_candi)
}
site_count <- function(site, site_hash) {
if(site_hash$has_key(site)) {
return(site_hash[[site]])
}
return(site)
}
sites <- sites_count$WORKSITE
sites_before <- c()
sites_after <- c()
count <- 0
for(site in sites) {
# Count of current Worksite
curr_count <- site_count(site,site_hash)
#print(paste0(site, ", ",curr_count))
if(curr_count < 100) { # Threshold
#print(paste0(site, ", ",curr_count))
corrected <- site_spell_correcter(site,site_hash)
if(corrected != site) { # Correction occurred
count <- count + 1
sites_before[count] <- site
sites_after[count] <- corrected
corrected_count <- site_count(corrected,site_hash)
#print(paste0(site, " : ", curr_count,", ",corrected, " : ", corrected_count))
}
}
}
sites_corrected_hash <- hashmap(sites_before,sites_after)
worksite_correct <- function(x, hash) {
if(hash$has_key(x)) {
return(hash[[x]])
}
return(x)
}
h1b$WORKSITE_CORRECTED <- sapply(h1b$WORKSITE,worksite_correct,hash=sites_corrected_hash)
h1b %>%
select(-WORKSITE) %>%
rename(WORKSITE = WORKSITE_CORRECTED) -> h1b
head(h1b)
h1b %>%
select(-WORKSITE_STATE_FULL) -> h1b_compact
save(h1b_compact, file = "h1b_transformed.RData")
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
df <- readRDS('./h1b_transformed.rds')
load("./data/cleaned_data.Rdata",envir=.GlobalEnv)
df <- readRDS("./h1b_transformed.rds")
setwd("D://")
df <- readRDS("./h1b_transformed.rds")
runApp('h1b_visa_shiny')
setwd("D:/")
df <- readRDS("./h1b_transformed.rds")
runApp('h1b_visa_shiny')
runApp('h1b_visa_shiny')
runApp('h1b_visa_shiny')
runApp('h1b_visa_shiny')
map_gen <- function(df,metric,USA,...) {
# Function to generate map plot for given metric in df
# This is laid on top of USA map
# Inputs:
# df      : dataframe with metrics, lat, lon, WORKSITE columns
# metric  : metric for data comparison
# USA     : dataframe for US maps with lat, long columns. map_data(map = "usa") from ggplot2
# Output  : ggplot object
# Creating Map Dataframe
df %>%
mutate(certified =ifelse(CASE_STATUS == "CERTIFIED",1,0)) %>%
group_by(WORKSITE,lat,lon) %>%
summarise(TotalApps = n(),CertiApps = sum(certified), Wage = median(PREVAILING_WAGE)) -> map_df
# # Lat-Long Limits
# df %>%
#   summarise(lat_min = min(lat,na.rm=TRUE),
#             lat_max = max(lat,na.rm=TRUE),
#             long_min = min(lon,na.rm=TRUE),
#             long_max = max(lon,na.rm=TRUE)) -> geo_coord
# Finding top Locations for metric
top_locations <- unlist(find_top(df,"WORKSITE",metric, ...))
# First layer    : USA Map
# Second layer   : geom_point() with point alpha and size varying with metric
# Third layer    : points mapping to top locations using ggrepel package
g <- ggplot(USA, aes(x=long, y=lat)) +
geom_polygon() + xlab("Longitude (deg)") + ylab("Latitude(deg)") +
geom_point(data=map_df, aes_string(x="lon", y="lat", label = "WORKSITE", alpha = metric, size = metric), color="yellow") +
geom_label_repel(data=map_df %>% filter(WORKSITE %in% top_locations),aes_string(x="lon", y="lat",label = "WORKSITE"),
fontface = 'bold', color = 'black',
box.padding = unit(0.0, "lines"),
point.padding = unit(1.0, "lines"),
segment.color = 'grey50',
force = 3) +
# Zoom into the specific location input
#coord_map(ylim = c(max(geo_coord$lat_min - 5,23), min(geo_coord$lat_max - 5,50)),xlim=c(max(geo_coord$long_min - 5,-130),min(geo_coord$long_max + 5,-65))) +
# Using the whole USA map
coord_map(ylim = c(23,50),xlim=c(-130,-65)) +
get_theme()
return(g)
}
shinyApp(ui,server)
shinyApp(shinyUI,shinyServer)
runApp('h1b_visa_shiny')
runApp('h1b_visa_shiny')
runApp('h1b_visa_shiny')
runApp('h1b_visa_shiny')
df <- load('D:/h1b_transformed.rds')
df <- readRDS('D:/h1b_transformed.rds')
runApp('h1b_visa_shiny')
runApp('h1b_visa_shiny')
setwd("D:/Spr2017-Proj5-SiyuanYao")
df <- readRDS('D:/Spr2017-Proj5-SiyuanYao/data/h1b_transformed.rds')
runApp('D:/h1b_visa_shiny')
runApp('D:/h1b_visa_shiny')
df <- readRDS('D:/Spr2017-Proj5-Siyuan Yao/h1b_transformed.rds')
runApp('D:/h1b_visa_shiny')
source("helpers.R")
df <- readRDS('D:/h1b_transformed.rds')
source("helpers.R")
runApp('D:/h1b_visa_shiny')
runApp('doc')
runApp('doc')
source("D:/Spr2017-Proj5-SiyuanYao/doc/helpers.R")
setwd("D:/Spr2017-Proj5-SiyuanYao")
df <- readRDS('./data/h1b_transformed.rds')
runApp('doc')
runApp('doc')
runApp('doc')
runApp('doc')
install.packages("curl")
install.packages("htmltools")
install.packages("jsonlite")
install.packages("stringi")
install.packages("tibble")
